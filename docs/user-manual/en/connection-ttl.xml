<?xml version="1.0" encoding="UTF-8"?>
<!-- ============================================================================= -->
<!-- Copyright © 2009 Red Hat, Inc. and others.                                    -->
<!--                                                                               -->
<!-- The text of and illustrations in this document are licensed by Red Hat under  -->
<!-- a Creative Commons Attribution–Share Alike 3.0 Unported license ("CC-BY-SA"). -->
<!--                                                                               -->
<!-- An explanation of CC-BY-SA is available at                                    -->
<!--                                                                               -->
<!--            http://creativecommons.org/licenses/by-sa/3.0/.                    -->
<!--                                                                               -->
<!-- In accordance with CC-BY-SA, if you distribute this document or an adaptation -->
<!-- of it, you must provide the URL for the original version.                     -->
<!--                                                                               -->
<!-- Red Hat, as the licensor of this document, waives the right to enforce,       -->
<!-- and agrees not to assert, Section 4d of CC-BY-SA to the fullest extent        -->
<!-- permitted by applicable law.                                                  -->
<!-- ============================================================================= -->
<chapter id="connection-ttl">
    <title>Dead Connections and Session Multiplexing</title>
    <para>In this section we will discuss connection time-to-live (TTL) and explain how HornetQ
        deals with crashed clients and clients which have exited without cleanly closing their
        resources. We'll also discuss how HornetQ multiplexes several sessions on a single
        connection.</para>
    <section id="dead.connections">
        <title>Cleaning up Dead Connection Resources on the Server</title>
        <para>Before a HornetQ client application exits it is considered good practice that it
            should close its resources in a controlled manner, using a <literal>finally</literal>
            block.</para>
        <para>Here's an example of a well behaved core client application closing its session and
            session factory in a finally block:</para>
        <programlisting>
ClientSessionFactory sf = null;
ClientSession session = null;

try
{
   sf = new ClientSessionFactoryImpl(...);

   session = sf.createSession(...);
   
   ... do some stuff with the session...
}
finally
{
   if (session != null)
   {
      session.close();
   }
   
   if (sf != null)
   {
      sf.close();
   }
}
        </programlisting>
        <para>And here's an example of a well behaved JMS client application:</para>
        <programlisting>
Connection jmsConnection = null;

try
{
   ConnectionFactory jmsConnectionFactory = new HornetQConnectionFactory(...);

   jmsConnection = jmsConnectionFactory.createConnection();

   ... do some stuff with the connection...
}
finally
{
   if (connection != null)
   {
      connection.close();
   }
}
        </programlisting>
        <para>Unfortunately users don't always write well behaved applications, and sometimes
            clients just crash so they don't have a chance to clean up their resources!</para>
        <para>If this occurs then it can leave server side resources, like sessions, hanging on the
            server. If these were not removed they would cause a resource leak on the server and
            over time this result in the server running out of memory or other resources.</para>
        <para>We have to balance the requirement for cleaning up dead client resources with the fact
            that sometimes the network between the client and the server can fail and then come
            back, allowing the client to reconnect. HornetQ supports client reconnection, so we
            don't want to clean up "dead" server side resources too soon or this will prevent any
            client from reconnecting, as it won't be able to find its old sessions on the
            server.</para>
        <para>HornetQ makes all of this configurable. For each <literal
                >ClientSessionFactory</literal> we define a <emphasis>connection TTL</emphasis>.
            Basically, the TTL determines how long the server will keep a connection alive in the
            absence of any data arriving from the client. The client will automatically send "ping"
            packets periodically to prevent the server from closing it down. If the server doesn't
            receive any packets on a connection for the connection TTL time, then it will
            automatically close all the sessions on the server that relate to that
            connection.</para>
        <para>If you're using JMS, the connection TTL is defined by the <literal
                >ConnectionTTL</literal> attribute on a <literal>HornetQConnectionFactory</literal>
            instance, or if you're deploying JMS connection factory instances direct into JNDI on
            the server side, you can specify it in the xml config, using the parameter <literal
                >connection-ttl</literal>.</para>
        <para>The default value for connection ttl is <literal>60000</literal>ms, i.e. 1 minute. A
            value of <literal>-1</literal> for <literal>ConnectionTTL</literal> means the server
            will never time out the connection on the server side.</para>
        <para>If you do not wish clients to be able to specify their own connection TTL, you can
            override all values used by a global value set on the server side. This can be done by
            specifying the <literal>connection-ttl-override</literal> attribute in the server side
            configuration. The default value for <literal>connection-ttl-override</literal> is
                <literal>-1</literal> which means "do not override" (i.e. let clients use their own
            values).</para>
        <section>
            <title>Closing core sessions or JMS connections that you have failed to close</title>
            <para>As previously discussed, it's important that all core client sessions and JMS
                connections are always closed explicitly in a <literal>finally</literal> block when
                you are finished using them. </para>
            <para>If you fail to do so, HornetQ will detect this at garbage collection time, and log
                a warning similar to the following in the logs (If you are using JMS the warning
                will involve a JMS connection not a client session):</para>
            <programlisting>
                
[Finalizer] 20:14:43,244 WARNING [org.hornetq.core.client.impl.DelegatingSession]  I'm closin
g a ClientSession you left open. Please make sure you close all ClientSessions explicitly before let
ting them go out of scope!
[Finalizer] 20:14:43,244 WARNING [org.hornetq.core.client.impl.DelegatingSession]  The sessi
on you didn't close was created here:
java.lang.Exception
at org.hornetq.core.client.impl.DelegatingSession.&lt;init&gt;(DelegatingSession.java:83)
at org.acme.yourproject.YourClass (YourClass.java:666)    
                
            </programlisting>
            <para>HornetQ will then close the connection / client session for you.</para>
            <para>Note that the log will also tell you the exact line of your user code where you
                created the JMS connection / client session that you later did not close. This will
                enable you to pinpoint the error in your code and correct it appropriately.</para>
        </section>
    </section>
    <section>
        <title>Detecting failure from the client side.</title>
        <para>In the previous section we discussed how the client sends pings to the server and how
            "dead" connection resources are cleaned up by the server. There's also another reason
            for pinging, and that's for the <emphasis>client</emphasis> to be able to detect that
            the server or network has failed.</para>
        <para>As long as the client is receiving data from the server it will consider the
            connection to be still alive. </para>
        <para>If the client does not receive any packets for <literal
                >client-failure-check-period</literal> milliseconds then it will consider the
            connection failed and will either initiate failover, or call any <literal
                >FailureListener</literal> instances (or <literal>ExceptionListener</literal>
            instances if you are using JMS) depending on how it has been configured.</para>
        <para>If you're using JMS it's defined by the <literal>ClientFailureCheckPeriod</literal>
            attribute on a <literal>HornetQConnectionFactory</literal> instance, or if you're
            deploying JMS connection factory instances direct into JNDI on the server side, you can
            specify it in the <literal>hornetq-jms.xml </literal> configuration file, using the
            parameter <literal>client-failure-check-period</literal>.</para>
        <para>The default value for client failure check period is <literal>30000</literal>ms, i.e. 30
            seconds. A value of <literal>-1</literal> means the client will never fail the
            connection on the client side if no data is received from the server. Typically this is
            much lower than connection TTL to allow clients to reconnect in case of transitory
            failure.</para>
    </section>
    <section id="connection-ttl.async-connection-execution">
        <title>Configuring Asynchronous Connection Execution</title>
        <para>By default, as packets are received by on the server side, they are handed off to a
            thread from the internal thread pool for processing, rather than processing them on the
            remoting thread.</para>
        <para>This prevents remoting threads being tied up for too long, especially if the operation
            takes a significant time to complete. It's dangerous for remoting threads to be tied up
            for too long, since then they might not be able to handle pings from the client,
            resulting in reply pings being sent back to the client late and the client erroneously
            thinking a problem has happened on the connection.</para>
        <para>Processing operations asynchronously on another thread does however add a little more
            latency, so we allow this to be configured using the parameter <literal
                >async-connection-execution-enabled</literal> in <literal
                >hornetq-configuration.xml</literal>. The default value for this parameter is
                <literal>true</literal>.</para>
        <para>If you do set this parameter to <literal>false</literal> please do so with
            caution.</para>
    </section>
    <section id="connection-ttl.session.multiplexing">
        <title>Session Multiplexing</title>
        <para>Each <literal>ClientSessionFactory</literal> creates connections on demand to the same
            server as you create sessions. Each instance will create up to a maximum of <literal
                >maxConnections</literal> connections to the same server. Subsequent sessions will
            use one of the already created connections in a round-robin fashion.</para>
        <para>To illustrate this, let's say <literal>maxConnections</literal> is set to <literal
                >8</literal>. The first eight sessions that you create will have a new underlying
            connection created for them, the next eight you create will use one of the previously
            created connections.</para>
        <para>The default value for <literal>maxConnections</literal> is <literal>8</literal>, if
            you prefer you can set it to a lower value so each factory maintains only one underlying
            connection. We choose a default value of <literal>8</literal> because on the server side
            each packet read from a particular connection is read serially by the same thread, so,
            if all traffic from the clients sessions is multiplexed on the same connection it will
            all be processed by the same thread on the server, which might not be a good use of
            cores on the server. By choosing <literal>8</literal> then different sessions traffic
            from the same client can be processed by different cores. If you have many different
            clients then this may not be relevant anyway.</para>
        <para>To change the value of <literal>maxConnections</literal> simply use the setter method
            on the <literal>ClientSessionFactory</literal> immediately after constructing it, or if
            you are using JMS use the setter on the <literal>HornetQConnectionFactory</literal> or
            specify the <literal>max-connections</literal> parameter in the connection factory xml
            configuration in <literal>hornetq-jms.xml</literal>.</para>
    </section>
</chapter>
